# -*- coding: utf-8 -*-
"""MultivariateStatistics

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IsvhQpAaLHz7Z0CNTQb52Y6Id8p3gHfF

# Salary Data

1) https://www.kaggle.com/datasets/mohithsairamreddy/salary-data (link of source)

## About Dataset
The dataset was obtained from multiple sources, including surveys, job posting sites, and other publicly available sources.A total of 6704 data points were collected.The dataset included five variables: age, experience, job role, and education level and salary.

### 2) EDA (for describing the database)
"""

import pandas as pd

df = pd.read_excel("Salary_DataNewX.xlsx")

df

new_header = df.iloc[0] #grab the first row for the header
df = df[1:] #take the data less the header row
df.columns = new_header #set the header row as the df header

df

df.columns = df.columns.str.replace(' ', '_')

df.dtypes

df[['Age', 'Years_of_Experience', 'Salary', 'Bonus', 'NumOfWorkplaces']] = df[['Age', 'Years_of_Experience', 'Salary', 'Bonus', 'NumOfWorkplaces']].apply(pd.to_numeric)

df.dtypes

df.isna().sum()

df[df.isna().any(axis=1)]

df.describe()

def frequency_table(x):
    return pd.crosstab(index=x,  columns="count")

frequency_table(df['Gender']) #frequency table for Gender

frequency_table(df['Education_Level'])

df.nunique()

df.loc[df['Education_Level'] == "Bachelor's", 'Education_Level'] = "Bachelor's Degree"
df.loc[df['Education_Level'] == "Master's", 'Education_Level'] = "Master's Degree"
df.loc[df['Education_Level'] == "phD", 'Education_Level'] = "PhD"

frequency_table(df['Education_Level'])

df['Age'] = df['Age'].fillna(df['Age'].mean())
df['Gender'] = df['Gender'].fillna(method='bfill')
df['Education_Level'] = df['Education_Level'].fillna(df['Education_Level'].mode()[0])
df['Job_Title'] = df['Job_Title'].fillna(df['Job_Title'].mode()[0])
df['Years_of_Experience'] = df['Years_of_Experience'].fillna(df['Years_of_Experience'].mean())
df['Salary'] = df['Salary'].fillna(df['Salary'].mean())
df['Bonus'] = df['Bonus'].fillna(df['Years_of_Experience'] * 0.8)
df['NumOfWorkplaces'] = df['NumOfWorkplaces'].fillna(df['NumOfWorkplaces'].mean())

df.isna().sum()

df['Job_Title'].mode()

import seaborn as sns

sns.pairplot(df, hue='Education_Level')

"""The diagonal plots are useful for understanding the distributional properties of each variable, including information such as central tendency, spread, and skewness. They can help identify any outliers, multimodality, or deviations from normality in the variables.

The hue parameter is optional and specifies a categorical variable to use for coloring or grouping the data points.

3)
**Diagonal Plots:** Look at the diagonal plots in the pairplot matrix. If the histograms or kernel density plots are bell-shaped and symmetrical, it suggests that the variable may follow a Gaussian distribution. However, it's important to note that the shape of the distribution may vary depending on the sample size.

**Scatter Plots:** Examine the scatter plots where the variable of interest is plotted against other variables. If the data points appear to form a symmetric, elliptical cloud around the mean, it indicates a linear relationship and may suggest a Gaussian distribution.

**Outliers:** Check for the presence of outliers in the scatter plots. Extreme values that deviate significantly from the main cluster of points could indicate a departure from a Gaussian distribution.

**Normal Probability Plot:** While not directly related to pairplots, a normal probability plot (Q-Q plot) can be used to assess the normality of a single variable. If the data points in the Q-Q plot closely follow a straight line, it suggests a Gaussian distribution.

##Also we can do Q_Q plot
"""

import statsmodels.api as sm
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 3, figsize=(15, 5))
# Q-Q plot for 'Age'
sm.qqplot(df['Age'], line='s', ax=axes[0])
axes[0].set_title("Q-Q Plot for Age")

# Q-Q plot for 'Experience'
sm.qqplot(df['Years_of_Experience'], line='s', ax=axes[1])
axes[1].set_title("Q-Q Plot for Experience")

# Q-Q plot for 'Salary'
sm.qqplot(df['Salary'], line='s', ax=axes[2])
axes[2].set_title("Q-Q Plot for Salary")
# Q-Q plot for 'Bonus'
sm.qqplot(df['Bonus'], line='s', ax=axes[3])
axes[3].set_title("Q-Q Plot for Bonus")
# Q-Q plot for 'NumOfWorkplaces'
sm.qqplot(df['NumOfWorkplaces'], line='s', ax=axes[4])
axes[4].set_title("Q-Q Plot for NumOfWorkplaces")

# Adjust spacing between subplots
plt.tight_layout()

# Show the plots
plt.show()

"""**Linearity:** If the points on the Q-Q plot form a reasonably straight line without significant deviations, it suggests that the variable is approximately normally distributed.

**Symmetry:** The points on the Q-Q plot should be symmetrically distributed around the straight line. This indicates that the variable has a symmetric distribution, which is a characteristic of a normal distribution.

Endpoints: The alignment of the points at the endpoints of the Q-Q plot can provide insights. If the points at both ends align closely to the line, it suggests that the tails of the distribution are normally distributed as well.

While visual inspection can provide initial indications, it is important to note that it does not provide a definitive proof of normality. For a more rigorous assessment, statistical tests can be performed, such as the Shapiro-Wilk test or the Anderson-Darling test. These tests provide statistical evidence of normality based on the p-value, where a higher p-value indicates stronger evidence for normality.

### 5) Let's choose **'Salary'** variable as target variable. In this dataset the main interest is to predict **Salary** based on rest of features: predictors variables (Age, Gender, Experience, Education, Job).

# 6) Linear Regression

To perform linear regression with the target variable **Salary** and predictor variables **Age, Gender, Years of Experience, Job Title, and Education Level,** you need to encode the categorical variables (Gender, Job Title, and Education Level) using one-hot encoding before fitting the linear regression model.
"""

from sklearn.preprocessing import LabelEncoder

# Create a label encoder object
label_encoder = LabelEncoder()

# Apply label encoding to a categorical column
df['Gender'] = label_encoder.fit_transform(df['Gender'])
df['Job_Title'] = label_encoder.fit_transform(df['Job_Title'])
df['Education_Level'] = label_encoder.fit_transform(df['Education_Level'])

df.dtypes

import statsmodels.api as sm
import pandas as pd

# Load the data
salary = df.copy()

# Create the linear regression model
X = salary[['Age', 'Years_of_Experience', 'NumOfWorkplaces']]  # Independent variables
y = salary['Salary']           # Dependent variable

# Add a constant term to the independent variables
X = sm.add_constant(X)

# Fit the linear regression model
salary_model = sm.OLS(y, X).fit()

# Print the model summary
print(salary_model.summary())

print(salary_model.params)

"""salary ≈ 100357.611185 -1746.980937 × Age + 9105.274961 × Years_of_Experience + 1738.2336 × NumOfWorkplaces

https://towardsdatascience.com/how-to-estimate-salary-with-linear-regression-6e5511db28f6
"""

import numpy as np
n = df.shape[0]
p = len(salary_model.params)
X = np.column_stack((np.ones(n), df['Age'], df['Years_of_Experience'], df['NumOfWorkplaces']))
y = df['Salary']
beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
print(beta_hat)

df

# Calculate the correlation matrix
corr_matrix = df.corr()

# Plot the correlation circle
plt.figure(figsize=(8, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)

plt.title('Correlation Circle')
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)

# Perform PCA
pca = PCA()
pca.fit(df_scaled)

# Get the transformed data
df_pca = pca.transform(df_scaled)

# Calculate the correlation matrix of the transformed data
corr_matrix = np.corrcoef(df_pca.T)

# Plot the correlation circle
plt.figure(figsize=(8, 8))
plt.scatter(df_pca[:, 0], df_pca[:, 1], alpha=0.5)
for i, (var_x, var_y) in enumerate(zip(corr_matrix[:, 0], corr_matrix[:, 1])):
    plt.arrow(0, 0, var_x, var_y, color='r', alpha=0.5)
    plt.text(var_x * 1.1, var_y * 1.1, df.columns[i], color='g', ha='center', va='center')

plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Correlation Circle (PC1 vs PC2)')
plt.grid(True)
plt.show()

# Standardize the data
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)

# Perform PCA with two components
pca = PCA(n_components=2)
pca.fit(df_scaled)

# Get the transformed data
df_pca = pca.transform(df_scaled)

# Calculate the correlation matrix of the transformed data
corr_matrix = np.corrcoef(df_pca.T)

# Plot the correlation circle
plt.figure(figsize=(8, 8))
plt.scatter(df_pca[:, 0], df_pca[:, 1], alpha=0.5)
for i, (var_x, var_y) in enumerate(zip(corr_matrix[:, 0], corr_matrix[:, 1])):
    plt.arrow(0, 0, var_x, var_y, color='r', alpha=0.5)
    plt.text(var_x * 1.1, var_y * 1.1, df.columns[i], color='g', ha='center', va='center')

plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Correlation Circle (PC1 vs PC2)')
plt.grid(True)
plt.show()

from sklearn.decomposition import PCA
import numpy as np

data = df.copy()
# Perform PCA
pca = PCA()
data_features = data.drop('Salary', axis=1)  # Selecting all columns except 'Salary'
data_scaled = (data_features - data_features.mean()) / data_features.std()  # Scaling the data
pca.fit(data_scaled)

# Get eigenvalues
eig_vals = pca.explained_variance_

# Print eigenvalues
print(eig_vals)
pca.components_[0]

from sklearn.decomposition import PCA
import numpy as np

data = df.copy()

# Perform PCA
pca = PCA()
data_features = data.drop('Salary', axis=1)  # Selecting all columns except 'Salary'
data_scaled = (data_features - data_features.mean()) / data_features.std()  # Scaling the data
pca.fit(data_scaled)

# Get loadings (coefficients)
loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
explained_variances = pca.explained_variance_

# Print feature names and coefficients
for i, feature in enumerate(data_features.columns):
    print("Feature:", feature)
    print("Coefficient:", loadings[:, i])
    print()

print("Age mean:", df['Age'].mean())
print("Years_of_Experience mean:", df['Years_of_Experience'].mean())
print("NumOfWorkplaces mean:", df['NumOfWorkplaces'].mean())

from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt

data = df.copy()

# Perform PCA
pca = PCA()
data_features = data.drop('Salary', axis=1)  # Selecting all columns except 'Salary'
data_scaled = (data_features - data_features.mean()) / data_features.std()  # Scaling the data
pca.fit(data_scaled)

# Get loadings (coefficients)
loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
explained_variances = pca.explained_variance_

# Plot PCA variances
plt.figure(figsize=(8, 6))
plt.bar(range(1, len(explained_variances) + 1), explained_variances, color='black')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.title('PCA Variance Explained')
plt.show()

# Plot correlation circle
plt.figure(figsize=(8, 8))
for i, feature in enumerate(data_features.columns):
    plt.arrow(0, 0, loadings[i, 0], loadings[i, 1], head_width=0.1, head_length=0.1, fc='black', ec='black')
    plt.text(loadings[i, 0]*1.1, loadings[i, 1]*1.1, feature, color='black')
plt.xlim(-1, 1)
plt.ylim(-1, 1)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Correlation Circle')
plt.axhline(0, linestyle='--')
plt.axvline(0, linestyle='--')
plt.grid(linestyle='--', linewidth=0.5)
plt.show()

from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt

data = df.copy()

# Perform PCA
pca = PCA(n_components=2)  # Specify the number of components as 2
data_features = data.drop('Salary', axis=1)  # Selecting all columns except 'Salary'
data_scaled = (data_features - data_features.mean()) / data_features.std()  # Scaling the data
data_projected = pca.fit_transform(data_scaled)  # Project the data onto the first two principal components

# Plot the data projection
plt.figure(figsize=(8, 6))
plt.scatter(data_projected[:, 0], data_projected[:, 1])
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Data Projection in First Two Principal Components')
plt.show()

